package cve

import (
	"context"
	"log"
	"strconv"
	"strings"
	"time"

	"github.com/segmentio/kafka-go"
	"github.com/segmentio/kafka-go/sasl/scram"
)

type KafkaProducer interface {
	InitTopic()
	BatchSize() int
	Produce(cves *[]*[]byte) error
	Close()
}

type kafkaProducer struct {
	count int
	batchSize int
	topic string
	conn  *kafka.Conn
	client *kafka.Client
	writer *kafka.Writer
}

func NewKafkaProducer() KafkaProducer {
	network := GetEnvOrDefault("KAFKA_NETWORK", "tcp")
	topic := GetEnvOrDefault("KAFKA_TOPIC", "cve")
	brokersEnv := GetEnvOrDefault("KAFKA_BROKERS", "localhost:9092,localhost:9093,localhost:9094")
	brokers := strings.Split(brokersEnv, ",")
	primary := GetEnvOrDefault("KAFKA_PRIMARY", brokers[0])

	dialer := &kafka.Dialer{
		Timeout:       10 * time.Second,
		DualStack:     true,
	}

	username := GetEnvOrDefault("KAFKA_USERNAME", "")
	password := GetEnvOrDefault("KAFKA_PASSWORD", "")
	if username != "" || password != "" {
		auth, err := scram.Mechanism(scram.SHA512, username, password)
		if err != nil {
			log.Fatalf("Failed to create auth: %v", err)
		}
		dialer.SASLMechanism = auth
	}
	
	kafka.DefaultDialer = dialer
	conn, err := kafka.Dial(network, primary)
	if err != nil {
		log.Fatalf("Failed to connect to kafka: %v", err)
	}

	batchMaxBytes, _ := strconv.ParseInt(GetEnvOrDefault("KAFKA_MAX_SIZE", "10000000"), 10, 64)
	batchSize, _ := strconv.Atoi(GetEnvOrDefault("KAFKA_BATCH_SIZE", "2000"))
	batchTimeout, _ := strconv.Atoi(GetEnvOrDefault("KAFKA_BATCH_TIMEOUT_MILLIS", "10"))

	writer := &kafka.Writer{
		Addr: kafka.TCP(brokers...), 
		Topic: topic,
		BatchTimeout: time.Millisecond * time.Duration(batchTimeout),
		BatchSize: batchSize,
		BatchBytes: batchMaxBytes,
	}

	client := &kafka.Client{
		Addr: kafka.TCP(brokers...),
		Timeout: 10 * time.Second,
	}

	if dialer.SASLMechanism != nil {
		transport := &kafka.Transport{
			SASL: dialer.SASLMechanism,
		}
		writer.Transport = transport
		client.Transport = transport
	}

	return &kafkaProducer{
		conn: conn,
		client: client,
		topic: topic,
		writer: writer,
		count: 0,
		batchSize: batchSize,
	}
}

func (k *kafkaProducer) InitTopic() {
	topic := GetEnvOrDefault("KAFKA_TOPIC", "cve")
	partitions, err1 := strconv.Atoi(GetEnvOrDefault("KAFKA_TOPIC_PARTITIONS", "3"))
	replication, err2 := strconv.Atoi(GetEnvOrDefault("KAFKA_TOPIC_REPLICATION", "3"))
	if err1 != nil || err2 != nil {
		log.Fatalf("Failed to parse env: %v, %v", err1, err2)
	}

	topicConfigs := []kafka.TopicConfig{
		{
			Topic:             topic,
			NumPartitions:     partitions,
			ReplicationFactor: replication,
		},
	}

	err := k.conn.CreateTopics(topicConfigs...)
	if err != nil {
		log.Fatalf("Failed to create topic: %v", err)
	}
	waitTopicCreated(k.client, topic)

	log.Printf("Created topic: %v", topic)
}

// Waits 5 seconds to validate that the topic was created
func waitTopicCreated(client *kafka.Client, topic string) {
	for i := 0; i < 50; i++ {
		r, err := client.Fetch(context.Background(), &kafka.FetchRequest{
			Topic:     topic,
			Partition: 0,
			Offset:    0,
		})
		if err == nil && r.Error == nil {
			break
		}
		time.Sleep(100 * time.Millisecond)
	}
}

func (k *kafkaProducer) Produce(cves *[]*[]byte) error {
	messages := make([]kafka.Message, len(*cves))
	for i, cve := range *cves {
		messages[i] = kafka.Message{
			Value: *cve,
		}
	}
	k.count += len(*cves)
	err := k.writer.WriteMessages(context.Background(), messages...)
	if k.count % k.batchSize == 0 {
		log.Printf("Produced %v messages", k.count)
	}
	return err
}

func (k *kafkaProducer) BatchSize() int {
	return k.batchSize
}

func (k *kafkaProducer) Close() {
	k.writer.Close()
	k.conn.Close()
}
